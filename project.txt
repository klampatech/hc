# AI Job Search System - Complete Implementation Plan

## Project Overview
Build an AI-powered job discovery engine that uses natural language queries and iterative refinement to search through 100,000 job postings. Optimize for result quality while minimizing token usage (target: <$10 total spend).

---

## System Architecture

### Core Components
1. **Data Loader** - Load jobs.jsonl, parse embeddings and structured fields
2. **Search Engine** - Hybrid vector similarity + structured filtering
3. **Refinement Engine** - Stateful conversation with incremental filters
4. **Demo Script** - Showcase 7+ queries with token tracking

### Technology Stack
- **Language:** Python 3.9+
- **Dependencies:**
  - `numpy` - vector operations
  - `openai` - embeddings and LLM calls
  - Standard library only (json, typing, dataclasses)
- **Models:**
  - `text-embedding-3-small` - query embeddings (matches dataset)
  - `gpt-4o-mini` - intent parsing, re-ranking

### Token Budget
- **Development:** ~$5-7 (testing, experimentation)
- **Per Query Runtime:** <500 tokens average
  - Query embedding: ~50 tokens
  - Intent classification: ~100 tokens (conditional)
  - Re-ranking: ~800 tokens (30% of queries only)
- **Total Project:** <$10

---

## Data Structure Reference

### jobs.jsonl Format
Each line is a JSON object with these key fields:

```python
{
    "id": "string",
    "apply_url": "string",
    "job_information": {
        "title": "string",
        "description": "string (HTML)"
    },
    "v7_processed_job_data": {
        "title": "string",
        "seniority_level": "string",  # entry, mid, senior, lead, etc.
        "employment_type": "string",  # full-time, part-time, contract
        "location": "string",
        "is_remote": boolean,
        "salary_min": number or null,
        "salary_max": number or null,
        "required_skills": ["string"],
        "embedding_explicit_vector": [1536 floats],
        "embedding_inferred_vector": [1536 floats],
        "embedding_company_vector": [1536 floats]
    },
    "v5_processed_company_data": {
        "company_name": "string",
        "industry": "string",
        "organization_type": "string",  # startup, nonprofit, enterprise, etc.
        "employee_count": "string",
        "funding_stage": "string"
    }
}
```

---

## Implementation Steps

### STEP 1: Project Setup (30 minutes)

**File structure:**
```
job-search/
├── data/
│   └── jobs.jsonl (download from Google Drive)
├── src/
│   ├── data_loader.py
│   ├── search_engine.py
│   ├── refinement_engine.py
│   └── utils.py
├── demo.py
├── README.md
├── requirements.txt
└── tokens_report.md
```

**Create requirements.txt:**
```
numpy>=1.24.0
openai>=1.12.0
python-dotenv>=1.0.0
```

**Setup instructions:**
1. Create `.env` file with `OPENAI_API_KEY=your_key_here`
2. Download jobs.jsonl from: https://drive.google.com/file/d/1RRVWYAvfb4hUus1hUDY1nPQUJGqpiBiq/view?usp=sharing
3. Place in `data/` directory

---

### STEP 2: Data Loader Implementation (1-2 hours)

**File: src/data_loader.py**

**Purpose:** Load and index the jobs dataset efficiently

**Key classes:**

```python
from dataclasses import dataclass
from typing import List, Optional, Dict
import numpy as np

@dataclass
class Job:
    """Represents a single job posting"""
    id: str
    title: str
    company: str
    description: str
    apply_url: str
    
    # Structured fields
    seniority: Optional[str]
    employment_type: Optional[str]
    location: Optional[str]
    is_remote: bool
    salary_min: Optional[float]
    salary_max: Optional[float]
    skills: List[str]
    
    # Company info
    industry: Optional[str]
    org_type: Optional[str]
    employee_count: Optional[str]
    funding_stage: Optional[str]
    
    # Embeddings (numpy arrays)
    embedding_explicit: np.ndarray
    embedding_inferred: np.ndarray
    embedding_company: np.ndarray
    
    def to_display_string(self) -> str:
        """Format job for display in results"""
        parts = [f"{self.title} - {self.company}"]
        if self.is_remote:
            parts.append("Remote")
        elif self.location:
            parts.append(self.location)
        if self.salary_min and self.salary_max:
            parts.append(f"${self.salary_min/1000:.0f}K-${self.salary_max/1000:.0f}K")
        if self.org_type:
            parts.append(self.org_type.title())
        return " | ".join(parts)

class JobDataset:
    """Loads and indexes the job dataset"""
    
    def __init__(self, filepath: str):
        self.jobs: List[Job] = []
        self.embeddings_explicit: np.ndarray = None
        self.embeddings_inferred: np.ndarray = None
        self.embeddings_company: np.ndarray = None
        self._load_jobs(filepath)
        self._build_embedding_matrices()
    
    def _load_jobs(self, filepath: str):
        """Load jobs from JSONL file"""
        # Implementation:
        # - Read line by line (memory efficient)
        # - Parse JSON
        # - Extract fields from nested structure
        # - Handle missing/null values gracefully
        # - Convert embedding lists to numpy arrays
        # - Limit to first 100K jobs if file is larger
        pass
    
    def _build_embedding_matrices(self):
        """Stack embeddings into matrices for batch operations"""
        # Create numpy matrices: (num_jobs, 1536)
        # self.embeddings_explicit = np.vstack([job.embedding_explicit for job in self.jobs])
        # Same for inferred and company
        pass
    
    def get_job_by_id(self, job_id: str) -> Optional[Job]:
        """Retrieve job by ID"""
        pass
    
    def __len__(self) -> int:
        return len(self.jobs)
```

**Implementation checklist:**
- [ ] Parse JSONL line by line (don't load all into memory at once)
- [ ] Handle missing/null fields gracefully (many jobs incomplete)
- [ ] Convert embedding lists `[float, float, ...]` to numpy arrays
- [ ] Validate embeddings are 1536-dimensional
- [ ] Print loading progress every 10K jobs
- [ ] Cache loaded dataset in memory (reload is expensive)

**Test with:**
```python
dataset = JobDataset("data/jobs.jsonl")
print(f"Loaded {len(dataset)} jobs")
print(f"Sample job: {dataset.jobs[0].to_display_string()}")
print(f"Embedding shapes: {dataset.embeddings_explicit.shape}")
```

---

### STEP 3: Search Engine Implementation (3-4 hours)

**File: src/search_engine.py**

**Purpose:** Core search logic using vector similarity + structured filters

**Key components:**

```python
from typing import List, Dict, Optional, Tuple
import numpy as np
from openai import OpenAI
from dataclasses import dataclass

@dataclass
class SearchFilters:
    """Structured filters extracted from query"""
    is_remote: Optional[bool] = None
    seniority: Optional[str] = None  # entry, mid, senior, lead, etc.
    location: Optional[str] = None
    min_salary: Optional[float] = None
    org_type: Optional[str] = None  # startup, nonprofit, enterprise
    industry: Optional[str] = None
    employment_type: Optional[str] = None  # full-time, contract, etc.

@dataclass
class SearchResult:
    """Search result with score"""
    job: Job
    score: float
    match_type: str  # "explicit", "inferred", "company", "hybrid"

class SearchEngine:
    """Hybrid search using embeddings + structured filters"""
    
    def __init__(self, dataset: JobDataset, openai_api_key: str):
        self.dataset = dataset
        self.client = OpenAI(api_key=openai_api_key)
        self.token_usage = {"total": 0, "by_operation": {}}
    
    def search(
        self, 
        query: str, 
        top_k: int = 10,
        use_reranking: bool = False
    ) -> Tuple[List[SearchResult], int]:
        """
        Main search function
        
        Returns: (results, tokens_used)
        """
        # STEP 1: Embed the query (50 tokens)
        query_embedding, tokens_1 = self._embed_query(query)
        
        # STEP 2: Extract structured filters (100 tokens)
        filters, tokens_2 = self._extract_filters(query)
        
        # STEP 3: Determine query type and embedding weights (0 tokens - rule-based)
        weights = self._determine_embedding_weights(query, filters)
        
        # STEP 4: Compute similarity scores (0 tokens - pure math)
        scores = self._compute_similarity_scores(query_embedding, weights)
        
        # STEP 5: Apply filters (0 tokens)
        filtered_indices = self._apply_filters(filters)
        
        # STEP 6: Get top candidates
        top_indices = self._get_top_k_indices(scores, filtered_indices, top_k * 3)
        candidates = [self.dataset.jobs[i] for i in top_indices]
        
        # STEP 7: Optional re-ranking (800 tokens, only for ambiguous queries)
        tokens_3 = 0
        if use_reranking or self._should_rerank(query, filters):
            candidates, tokens_3 = self._rerank_with_llm(query, candidates[:30], top_k)
        else:
            candidates = candidates[:top_k]
        
        # Create results
        results = [
            SearchResult(job=job, score=scores[idx], match_type="hybrid")
            for job, idx in zip(candidates, top_indices[:len(candidates)])
        ]
        
        total_tokens = tokens_1 + tokens_2 + tokens_3
        self._track_tokens("search", total_tokens)
        
        return results, total_tokens
    
    def _embed_query(self, query: str) -> Tuple[np.ndarray, int]:
        """
        Embed query using text-embedding-3-small
        
        Returns: (embedding_vector, tokens_used)
        """
        response = self.client.embeddings.create(
            model="text-embedding-3-small",
            input=query
        )
        embedding = np.array(response.data[0].embedding)
        tokens = response.usage.total_tokens
        return embedding, tokens
    
    def _extract_filters(self, query: str) -> Tuple[SearchFilters, int]:
        """
        Extract structured filters from query using GPT-4o-mini
        
        Use ~100 tokens to parse filters like:
        - "remote" → is_remote=True
        - "senior" → seniority="senior"
        - "NYC" or "New York" → location="New York"
        - "startup" → org_type="startup"
        - "over 150k" → min_salary=150000
        
        Returns: (filters, tokens_used)
        """
        system_prompt = """Extract structured filters from job search query.
Return ONLY a JSON object with these optional fields:
{
  "is_remote": true/false/null,
  "seniority": "entry"/"mid"/"senior"/"lead"/null,
  "location": "city name"/null,
  "min_salary": number/null,
  "org_type": "startup"/"nonprofit"/"enterprise"/null,
  "industry": "string"/null,
  "employment_type": "full-time"/"contract"/null
}

Examples:
"remote senior engineer" → {"is_remote": true, "seniority": "senior"}
"data scientist in NYC over 150k" → {"location": "New York", "min_salary": 150000}
"startup ML roles" → {"org_type": "startup"}"""

        response = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": query}
            ],
            temperature=0,
            max_tokens=150
        )
        
        filter_json = json.loads(response.choices[0].message.content)
        filters = SearchFilters(**filter_json)
        tokens = response.usage.total_tokens
        
        return filters, tokens
    
    def _determine_embedding_weights(
        self, 
        query: str, 
        filters: SearchFilters
    ) -> Dict[str, float]:
        """
        Determine weights for the 3 embedding types based on query intent
        
        Rules:
        - Explicit job criteria (title, skills) → high explicit weight
        - Company culture/mission queries → high company weight
        - Balanced queries → equal weights
        
        Returns: {"explicit": 0.5, "inferred": 0.3, "company": 0.2}
        """
        query_lower = query.lower()
        
        # Company-focused keywords
        company_keywords = [
            "startup", "culture", "mission", "nonprofit", "social good",
            "fast-growing", "early stage", "series", "funded"
        ]
        
        # Explicit job keywords
        explicit_keywords = [
            "engineer", "developer", "scientist", "analyst", "manager",
            "designer", "senior", "junior", "lead", "python", "react"
        ]
        
        company_score = sum(1 for kw in company_keywords if kw in query_lower)
        explicit_score = sum(1 for kw in explicit_keywords if kw in query_lower)
        
        if company_score > explicit_score:
            return {"explicit": 0.2, "inferred": 0.2, "company": 0.6}
        elif explicit_score > company_score * 2:
            return {"explicit": 0.7, "inferred": 0.2, "company": 0.1}
        else:
            return {"explicit": 0.5, "inferred": 0.3, "company": 0.2}
    
    def _compute_similarity_scores(
        self, 
        query_embedding: np.ndarray,
        weights: Dict[str, float]
    ) -> np.ndarray:
        """
        Compute weighted cosine similarity across all 3 embedding types
        
        Returns: array of scores, shape (num_jobs,)
        """
        # Normalize query embedding
        query_norm = query_embedding / np.linalg.norm(query_embedding)
        
        # Compute cosine similarity for each embedding type
        sim_explicit = self.dataset.embeddings_explicit @ query_norm
        sim_inferred = self.dataset.embeddings_inferred @ query_norm
        sim_company = self.dataset.embeddings_company @ query_norm
        
        # Weighted combination
        scores = (
            weights["explicit"] * sim_explicit +
            weights["inferred"] * sim_inferred +
            weights["company"] * sim_company
        )
        
        return scores
    
    def _apply_filters(self, filters: SearchFilters) -> np.ndarray:
        """
        Apply structured filters to get valid job indices
        
        Returns: boolean array, shape (num_jobs,)
        """
        mask = np.ones(len(self.dataset), dtype=bool)
        
        jobs = self.dataset.jobs
        
        if filters.is_remote is not None:
            mask &= np.array([j.is_remote == filters.is_remote for j in jobs])
        
        if filters.seniority:
            mask &= np.array([
                j.seniority and filters.seniority.lower() in j.seniority.lower() 
                for j in jobs
            ])
        
        if filters.location:
            mask &= np.array([
                j.location and filters.location.lower() in j.location.lower()
                for j in jobs
            ])
        
        if filters.min_salary:
            mask &= np.array([
                j.salary_min and j.salary_min >= filters.min_salary
                for j in jobs
            ])
        
        if filters.org_type:
            mask &= np.array([
                j.org_type and filters.org_type.lower() in j.org_type.lower()
                for j in jobs
            ])
        
        if filters.industry:
            mask &= np.array([
                j.industry and filters.industry.lower() in j.industry.lower()
                for j in jobs
            ])
        
        return mask
    
    def _get_top_k_indices(
        self, 
        scores: np.ndarray, 
        mask: np.ndarray, 
        k: int
    ) -> np.ndarray:
        """Get indices of top k jobs after applying mask"""
        scores_filtered = scores.copy()
        scores_filtered[~mask] = -np.inf
        return np.argsort(scores_filtered)[::-1][:k]
    
    def _should_rerank(self, query: str, filters: SearchFilters) -> bool:
        """
        Decide if query needs LLM re-ranking
        
        Skip re-ranking if query is straightforward:
        - Has clear structured filters (remote, seniority, location)
        - Uses common job titles
        
        Use re-ranking if query is ambiguous:
        - Cultural fit ("mission-driven", "innovative")
        - Vague requirements ("interesting problems")
        - Complex preferences ("growth opportunities")
        """
        query_lower = query.lower()
        
        # Ambiguous keywords that need LLM judgment
        ambiguous_keywords = [
            "culture", "mission", "innovative", "creative", "exciting",
            "growth", "opportunity", "interesting", "meaningful", "impact"
        ]
        
        # If ambiguous keywords present, rerank
        if any(kw in query_lower for kw in ambiguous_keywords):
            return True
        
        # If query is very short and has no filters, might be ambiguous
        if len(query.split()) <= 3 and not any([
            filters.is_remote, filters.seniority, filters.location, filters.org_type
        ]):
            return True
        
        return False
    
    def _rerank_with_llm(
        self, 
        query: str, 
        candidates: List[Job], 
        top_k: int
    ) -> Tuple[List[Job], int]:
        """
        Use GPT-4o-mini to re-rank top candidates
        
        Returns: (reranked_jobs, tokens_used)
        """
        # Create concise job descriptions for LLM
        job_summaries = []
        for i, job in enumerate(candidates):
            summary = f"{i}. {job.title} at {job.company}"
            if job.org_type:
                summary += f" ({job.org_type})"
            if job.is_remote:
                summary += " - Remote"
            if job.skills:
                summary += f" | Skills: {', '.join(job.skills[:5])}"
            job_summaries.append(summary)
        
        prompt = f"""Rank these {len(candidates)} jobs by relevance to: "{query}"

Jobs:
{chr(10).join(job_summaries)}

Return ONLY a JSON array of indices in ranked order (most relevant first):
[0, 5, 2, ...]

Return exactly {min(top_k, len(candidates))} indices."""

        response = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=200
        )
        
        ranked_indices = json.loads(response.choices[0].message.content)
        reranked_jobs = [candidates[i] for i in ranked_indices[:top_k]]
        tokens = response.usage.total_tokens
        
        return reranked_jobs, tokens
    
    def _track_tokens(self, operation: str, tokens: int):
        """Track token usage"""
        self.token_usage["total"] += tokens
        if operation not in self.token_usage["by_operation"]:
            self.token_usage["by_operation"][operation] = 0
        self.token_usage["by_operation"][operation] += tokens
    
    def get_token_report(self) -> str:
        """Generate token usage report"""
        report = f"Total tokens used: {self.token_usage['total']:,}\n\n"
        report += "By operation:\n"
        for op, tokens in self.token_usage["by_operation"].items():
            report += f"  {op}: {tokens:,} tokens\n"
        return report
```

**Implementation checklist:**
- [ ] Query embedding using text-embedding-3-small
- [ ] Filter extraction with minimal prompt (target: 100 tokens)
- [ ] Dynamic embedding weight calculation (0 tokens, rule-based)
- [ ] Efficient numpy cosine similarity (vectorized)
- [ ] Structured filter application (boolean masking)
- [ ] Conditional re-ranking (only when needed)
- [ ] Token tracking for all operations

**Test with:**
```python
engine = SearchEngine(dataset, api_key)

# Test 1: Simple query
results, tokens = engine.search("senior software engineer")
print(f"Tokens: {tokens}")
for r in results[:5]:
    print(r.job.to_display_string())

# Test 2: Complex query (should trigger reranking)
results, tokens = engine.search("mission-driven nonprofit data roles")
print(f"Tokens: {tokens}")
```

---

### STEP 4: Refinement Engine Implementation (2-3 hours)

**File: src/refinement_engine.py**

**Purpose:** Handle multi-turn conversations with state tracking

**Key components:**

```python
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass, field
import numpy as np

@dataclass
class ConversationState:
    """Track conversation state across turns"""
    active_filters: SearchFilters = field(default_factory=SearchFilters)
    last_query: str = ""
    last_query_embedding: Optional[np.ndarray] = None
    last_results: List[SearchResult] = field(default_factory=list)
    conversation_history: List[str] = field(default_factory=list)

class RefinementEngine:
    """Handle multi-turn search refinement"""
    
    def __init__(self, search_engine: SearchEngine):
        self.search_engine = search_engine
        self.client = search_engine.client
    
    def refine(
        self, 
        state: ConversationState, 
        new_query: str,
        top_k: int = 10
    ) -> Tuple[List[SearchResult], ConversationState, int]:
        """
        Refine search based on conversation context
        
        Returns: (results, updated_state, tokens_used)
        """
        # STEP 1: Detect if this is a refinement or pivot (200 tokens)
        intent, tokens_1 = self._parse_refinement_intent(state, new_query)
        
        # STEP 2: Handle based on intent
        if intent["type"] == "pivot":
            # New search from scratch
            results, tokens_2 = self.search_engine.search(new_query, top_k)
            
            # Reset state
            state = ConversationState(
                last_query=new_query,
                last_results=results,
                conversation_history=state.conversation_history + [new_query]
            )
        
        elif intent["type"] == "refinement":
            # Incremental refinement
            # Merge new filters with existing ones
            new_filters = intent["filters"]
            merged_filters = self._merge_filters(state.active_filters, new_filters)
            
            # Re-search with merged filters
            combined_query = f"{state.last_query} {new_query}"
            results, tokens_2 = self.search_engine.search(
                combined_query, 
                top_k,
                use_reranking=False  # Already filtered, no need to rerank
            )
            
            # Update state
            state.active_filters = merged_filters
            state.last_query = combined_query
            state.last_results = results
            state.conversation_history.append(new_query)
        
        else:  # "clarification" or "show_different"
            # Handle edge cases
            results = state.last_results
            tokens_2 = 0
        
        total_tokens = tokens_1 + tokens_2
        return results, state, total_tokens
    
    def _parse_refinement_intent(
        self, 
        state: ConversationState, 
        new_query: str
    ) -> Tuple[Dict, int]:
        """
        Determine if new query is a refinement, pivot, or other action
        
        Returns: (intent_dict, tokens_used)
        
        Intent types:
        - "refinement": Narrow down existing results
        - "pivot": Start new search
        - "show_different": Show alternate results
        - "clarification": User asking question
        """
        context = f"Previous query: {state.last_query}\n" if state.last_query else ""
        context += f"Active filters: {self._filters_to_string(state.active_filters)}\n" if state.active_filters else ""
        
        prompt = f"""{context}New user input: "{new_query}"

Classify the intent and extract any new filters.

Return JSON:
{{
  "type": "refinement" | "pivot" | "show_different" | "clarification",
  "filters": {{
    "is_remote": true/false/null,
    "seniority": "entry"/"mid"/"senior"/"lead"/null,
    "location": "string"/null,
    "min_salary": number/null,
    "org_type": "startup"/"nonprofit"/"enterprise"/null,
    "industry": "string"/null
  }}
}}

Examples:
"make it remote" → {{"type": "refinement", "filters": {{"is_remote": true}}}}
"actually show me marketing roles instead" → {{"type": "pivot", "filters": {{}}}}
"at nonprofits" → {{"type": "refinement", "filters": {{"org_type": "nonprofit"}}}}
"""

        response = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=200
        )
        
        intent = json.loads(response.choices[0].message.content)
        tokens = response.usage.total_tokens
        
        return intent, tokens
    
    def _merge_filters(
        self, 
        existing: SearchFilters, 
        new: Dict
    ) -> SearchFilters:
        """
        Merge new filters with existing ones
        
        Rules:
        - New filters override existing (recency bias)
        - Handle contradictions (e.g., "NYC" then "remote")
        """
        merged = SearchFilters(**{
            k: v for k, v in existing.__dict__.items() if v is not None
        })
        
        # Apply new filters (override existing)
        for key, value in new.items():
            if value is not None:
                setattr(merged, key, value)
        
        # Handle contradictions
        if merged.is_remote and merged.location:
            # Remote overrides specific location
            merged.location = None
        
        return merged
    
    def _filters_to_string(self, filters: SearchFilters) -> str:
        """Convert filters to readable string"""
        parts = []
        if filters.is_remote:
            parts.append("remote")
        if filters.seniority:
            parts.append(filters.seniority)
        if filters.location:
            parts.append(filters.location)
        if filters.org_type:
            parts.append(filters.org_type)
        if filters.min_salary:
            parts.append(f"${filters.min_salary/1000:.0f}K+")
        return ", ".join(parts) if parts else "none"
```

**Implementation checklist:**
- [ ] Intent classification (refinement vs pivot)
- [ ] Filter merging with contradiction handling
- [ ] State preservation across turns
- [ ] Minimal context passed to LLM (save tokens)
- [ ] Handle edge cases (show different, clarifications)

**Test with:**
```python
state = ConversationState()
engine = RefinementEngine(search_engine)

# Turn 1
results, state, tokens = engine.refine(state, "data science jobs")
print(f"Turn 1: {tokens} tokens, {len(results)} results")

# Turn 2 - refinement
results, state, tokens = engine.refine(state, "at nonprofits")
print(f"Turn 2: {tokens} tokens, {len(results)} results")

# Turn 3 - refinement
results, state, tokens = engine.refine(state, "make it remote")
print(f"Turn 3: {tokens} tokens, {len(results)} results")
```

---

### STEP 5: Demo Script Implementation (1-2 hours)

**File: demo.py**

**Purpose:** Showcase system with diverse queries and token tracking

**Complete demo script:**

```python
import os
from dotenv import load_dotenv
from src.data_loader import JobDataset
from src.search_engine import SearchEngine
from src.refinement_engine import RefinementEngine, ConversationState

def print_results(results, query, tokens, max_display=10):
    """Print search results"""
    print(f"\n{'='*80}")
    print(f"Query: \"{query}\"")
    print(f"Tokens used: {tokens}")
    print(f"Results ({len(results)} found, showing top {min(max_display, len(results))}):")
    print(f"{'='*80}")
    
    for i, result in enumerate(results[:max_display], 1):
        print(f"{i}. {result.job.to_display_string()}")
        print(f"   Score: {result.score:.4f}")
        if i < max_display:
            print()

def demo_search(engine):
    """Demonstrate single-turn searches"""
    print("\n" + "="*80)
    print("DEMO 1: SINGLE-TURN SEARCHES")
    print("="*80)
    
    queries = [
        "senior software engineer remote",
        "data science internships",
        "product manager roles at early stage startups",
        "backend engineer jobs in New York paying over 150k",
        "entry level design roles",
        "mission-driven nonprofit data roles",  # Should trigger reranking
    ]
    
    total_tokens = 0
    
    for query in queries:
        results, tokens = engine.search(query, top_k=10)
        print_results(results, query, tokens, max_display=5)
        total_tokens += tokens
        print(f"\nRunning total tokens: {total_tokens}")
    
    return total_tokens

def demo_refinement(engine):
    """Demonstrate multi-turn refinement"""
    print("\n" + "="*80)
    print("DEMO 2: MULTI-TURN REFINEMENT FLOWS")
    print("="*80)
    
    refinement_engine = RefinementEngine(engine)
    
    # Flow 1: Data science -> nonprofits -> remote
    print("\n" + "-"*80)
    print("Refinement Flow 1: Data Science → Nonprofits → Remote")
    print("-"*80)
    
    state = ConversationState()
    flow_tokens = 0
    
    queries = [
        "data science jobs",
        "at companies or non-profits that care about social good",
        "make it remote"
    ]
    
    for i, query in enumerate(queries, 1):
        results, state, tokens = refinement_engine.refine(state, query, top_k=10)
        print(f"\nTurn {i}: \"{query}\"")
        print(f"Tokens: {tokens}")
        print(f"Active filters: {refinement_engine._filters_to_string(state.active_filters)}")
        print(f"\nTop 5 results:")
        for j, result in enumerate(results[:5], 1):
            print(f"  {j}. {result.job.to_display_string()}")
        flow_tokens += tokens
    
    print(f"\nFlow 1 total tokens: {flow_tokens}")
    
    # Flow 2: ML engineer -> startups -> senior
    print("\n" + "-"*80)
    print("Refinement Flow 2: ML Engineer → Startups → Senior")
    print("-"*80)
    
    state = ConversationState()
    flow2_tokens = 0
    
    queries = [
        "machine learning engineer",
        "at early stage startups",
        "senior level only"
    ]
    
    for i, query in enumerate(queries, 1):
        results, state, tokens = refinement_engine.refine(state, query, top_k=10)
        print(f"\nTurn {i}: \"{query}\"")
        print(f"Tokens: {tokens}")
        print(f"Active filters: {refinement_engine._filters_to_string(state.active_filters)}")
        print(f"\nTop 5 results:")
        for j, result in enumerate(results[:5], 1):
            print(f"  {j}. {result.job.to_display_string()}")
        flow2_tokens += tokens
    
    print(f"\nFlow 2 total tokens: {flow2_tokens}")
    
    return flow_tokens + flow2_tokens

def main():
    """Main demo script"""
    # Load environment
    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("OPENAI_API_KEY not found in .env file")
    
    print("Loading dataset...")
    dataset = JobDataset("data/jobs.jsonl")
    print(f"Loaded {len(dataset)} jobs\n")
    
    print("Initializing search engine...")
    engine = SearchEngine(dataset, api_key)
    print("Ready!\n")
    
    # Run demos
    search_tokens = demo_search(engine)
    refinement_tokens = demo_refinement(engine)
    
    # Final report
    print("\n" + "="*80)
    print("FINAL TOKEN USAGE REPORT")
    print("="*80)
    print(f"\nSingle-turn searches: {search_tokens:,} tokens")
    print(f"Multi-turn refinements: {refinement_tokens:,} tokens")
    print(f"Total demo tokens: {search_tokens + refinement_tokens:,} tokens")
    print(f"\nEstimated cost (GPT-4o-mini):")
    print(f"  ${(search_tokens + refinement_tokens) * 0.15 / 1_000_000:.4f}")
    print(f"\nAverage per single-turn search: {search_tokens / 6:.0f} tokens")
    print(f"Average per refinement turn: {refinement_tokens / 6:.0f} tokens")
    
    print("\n" + engine.get_token_report())
    
    # Save detailed report
    with open("tokens_report.md", "w") as f:
        f.write("# Token Usage Report\n\n")
        f.write(f"## Summary\n\n")
        f.write(f"- **Total tokens used in demo:** {search_tokens + refinement_tokens:,}\n")
        f.write(f"- **Estimated cost:** ${(search_tokens + refinement_tokens) * 0.15 / 1_000_000:.4f}\n")
        f.write(f"- **Average per search:** {search_tokens / 6:.0f} tokens\n")
        f.write(f"- **Average per refinement:** {refinement_tokens / 6:.0f} tokens\n\n")
        f.write(f"## Detailed Breakdown\n\n")
        f.write(engine.get_token_report())
    
    print("\nToken report saved to tokens_report.md")

if __name__ == "__main__":
    main()
```

**Implementation checklist:**
- [ ] Run 6+ single-turn searches
- [ ] Run 2+ multi-turn refinement flows
- [ ] Display results clearly
- [ ] Track tokens per query
- [ ] Generate final token report
- [ ] Save report to tokens_report.md

---

### STEP 6: Utilities Implementation (30 minutes)

**File: src/utils.py**

**Purpose:** Helper functions

```python
import json
import numpy as np
from typing import Any, Dict

def safe_get(d: Dict, *keys, default=None) -> Any:
    """Safely get nested dictionary values"""
    for key in keys:
        if isinstance(d, dict):
            d = d.get(key, {})
        else:
            return default
    return d if d != {} else default

def parse_salary(salary_str: str) -> tuple[float, float]:
    """Parse salary string to min/max"""
    # Handle formats like "$100K-$150K", "$100,000", etc.
    pass

def normalize_location(location: str) -> str:
    """Normalize location strings"""
    # "New York, NY" -> "New York"
    # "San Francisco Bay Area" -> "San Francisco"
    pass

def cosine_similarity_batch(
    query: np.ndarray, 
    vectors: np.ndarray
) -> np.ndarray:
    """
    Compute cosine similarity between query and batch of vectors
    
    Args:
        query: shape (dim,)
        vectors: shape (n, dim)
    
    Returns: similarities, shape (n,)
    """
    query_norm = query / np.linalg.norm(query)
    vectors_norm = vectors / np.linalg.norm(vectors, axis=1, keepdims=True)
    return vectors_norm @ query_norm
```

---

### STEP 7: Documentation (1 hour)

**File: README.md**

**Template:**

```markdown
# AI Job Search System

An AI-powered job discovery engine that uses natural language queries and iterative refinement to search through 100,000 job postings.

## Quick Start

### Prerequisites
- Python 3.9+
- OpenAI API key

### Installation

1. Clone repository
2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Download dataset:
   - Download jobs.jsonl from: https://drive.google.com/file/d/1RRVWYAvfb4hUus1hUDY1nPQUJGqpiBiq/view
   - Place in `data/` directory

4. Create `.env` file:
   ```
   OPENAI_API_KEY=your_key_here
   ```

### Run Demo

```bash
python demo.py
```

This will:
- Load 100K job postings
- Run 6 single-turn searches
- Run 2 multi-turn refinement flows
- Generate token usage report

## Architecture

### Data Representation

The system uses **pre-computed embeddings** (already in dataset) for three aspects of each job:

1. **Explicit embedding** - Job title, listed skills, requirements
2. **Inferred embedding** - Related skills, relevant experience
3. **Company embedding** - Company characteristics, industry, culture

Additionally, structured fields are indexed for filtering:
- Remote status, location, seniority
- Salary range, employment type
- Company type (startup, nonprofit, enterprise)

### Search Strategy

**Hybrid approach combining vector similarity + structured filters:**

```
User Query
    ↓
[1] Embed query (50 tokens)
    ↓
[2] Extract structured filters (100 tokens)
    ↓
[3] Determine embedding weights (0 tokens - rule-based)
    ↓
[4] Compute similarity scores (0 tokens - pure math)
    ↓
[5] Apply filters (0 tokens - boolean masking)
    ↓
[6] Optional re-ranking (800 tokens - only for ambiguous queries)
    ↓
Top 10 Results
```

**Key optimization:** 70% of queries can be answered with embeddings + filters alone (no LLM re-ranking needed).

### Relevance Ranking

**Dynamic embedding weights based on query intent:**

- **Job-focused queries** (e.g., "senior ML engineer")
  - 70% explicit, 20% inferred, 10% company
  
- **Company-focused queries** (e.g., "mission-driven startup")
  - 20% explicit, 20% inferred, 60% company
  
- **Balanced queries** (e.g., "data science at nonprofits")
  - 50% explicit, 30% inferred, 20% company

**Conditional re-ranking:** Only invoke LLM re-ranking when query contains ambiguous terms like "culture", "mission", "exciting", etc.

### Refinement Engine

**Stateful conversation with incremental filtering:**

```python
State = {
    "active_filters": {remote: True, seniority: "senior"},
    "last_query": "data science jobs",
    "last_results": [...]
}
```

Each refinement:
1. Parse intent (200 tokens) - is this a refinement or pivot?
2. Merge new filters with existing ones
3. Re-search with combined context
4. Update state

**Contradiction handling:** Latest filter overrides (e.g., "NYC" then "remote" → remote wins)

## Trade-offs

### What We Optimized For
✅ Token efficiency (target: <500 tokens/query)
✅ Result quality for diverse query types
✅ Fast iteration (no complex infrastructure)
✅ Handling 100K job scale in-memory

### What We Deprioritized
❌ Sub-second latency (embeddings + LLM calls take 2-5s)
❌ Exact phrase matching (embeddings are semantic)
❌ Learning from user feedback (stateless between sessions)
❌ Production-grade error handling

## Query Performance

### Works Well ✅
- Clear job titles: "senior software engineer", "data analyst"
- Location + remote: "remote jobs in tech", "NYC product manager"
- Company types: "startup ML roles", "nonprofit data science"
- Salary filters: "engineering jobs over 150k"
- Multi-turn refinement: "data science" → "at nonprofits" → "remote"

### Challenging ⚠️
- Very vague queries: "interesting roles", "good culture"
- Hyper-specific tech stacks: "Python + Kafka + Airflow + dbt"
- Company names not in dataset
- Queries mixing many constraints (might over-filter)

## Token Usage

**Development:** ~$5-7
- Testing searches: 30 queries × 300 tokens = ~9K tokens
- Testing refinements: 20 turns × 400 tokens = ~8K tokens
- Total: ~17K tokens ≈ $2.55 with GPT-4o-mini

**Per-Query Runtime:**
- Simple search (with filters): ~150 tokens
- Ambiguous search (with re-ranking): ~950 tokens
- Refinement turn: ~350 tokens
- **Average: ~400 tokens/query**

**Demo script total:** ~3,500 tokens (~$0.50)

## Future Improvements

With more time, I would add:

1. **Caching**
   - Cache query embeddings for similar searches
   - Pre-compute common filter combinations
   - Estimated savings: 30-50% tokens

2. **Smarter re-ranking triggers**
   - ML model to predict when re-ranking helps
   - Could reduce re-ranking from 30% → 15% of queries

3. **User feedback loop**
   - Track which results users click
   - Fine-tune embedding weights per query type
   - Personalization based on past searches

4. **Hybrid retrieval**
   - BM25 for exact keyword matching
   - Combine with semantic search
   - Better for technical skill queries

5. **Batch processing**
   - Pre-compute cluster centroids
   - Approximate nearest neighbor search
   - Scale to 1M+ jobs

6. **Explanation generation**
   - Show why each job matched
   - Highlight relevant skills/requirements
   - Build trust with users

## Time Spent

**Total:** ~12 hours
- Data exploration & loading: 2h
- Search engine: 4h
- Refinement engine: 2h
- Demo & testing: 2h
- Documentation: 2h
```

**File: tokens_report.md** (generated by demo.py)

---

### STEP 8: Testing & Validation (1 hour)

**Create: tests.py** (optional but recommended)

```python
"""Basic tests to validate system works"""

def test_data_loading():
    """Test dataset loads correctly"""
    dataset = JobDataset("data/jobs.jsonl")
    assert len(dataset) > 0
    assert dataset.embeddings_explicit.shape[1] == 1536
    print("✓ Data loading works")

def test_search_basic():
    """Test basic search returns results"""
    dataset = JobDataset("data/jobs.jsonl")
    engine = SearchEngine(dataset, api_key)
    results, tokens = engine.search("software engineer", top_k=10)
    assert len(results) == 10
    assert tokens > 0
    assert tokens < 1000  # Should be efficient
    print(f"✓ Basic search works ({tokens} tokens)")

def test_search_with_filters():
    """Test structured filters work"""
    dataset = JobDataset("data/jobs.jsonl")
    engine = SearchEngine(dataset, api_key)
    results, tokens = engine.search("remote senior engineer", top_k=10)
    # Check that results respect filters
    remote_count = sum(1 for r in results if r.job.is_remote)
    assert remote_count > 5  # Most should be remote
    print(f"✓ Filtered search works ({remote_count}/10 remote)")

def test_refinement():
    """Test multi-turn refinement"""
    dataset = JobDataset("data/jobs.jsonl")
    engine = SearchEngine(dataset, api_key)
    ref_engine = RefinementEngine(engine)
    
    state = ConversationState()
    
    # Turn 1
    results1, state, tokens1 = ref_engine.refine(state, "data scientist")
    assert len(results1) == 10
    
    # Turn 2 - should narrow down
    results2, state, tokens2 = ref_engine.refine(state, "make it remote")
    assert len(results2) > 0
    assert state.active_filters.is_remote == True
    
    print(f"✓ Refinement works ({tokens1 + tokens2} tokens total)")

if __name__ == "__main__":
    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY")
    
    test_data_loading()
    test_search_basic()
    test_search_with_filters()
    test_refinement()
    
    print("\nAll tests passed! ✓")
```

**Run tests before finalizing:**
```bash
python tests.py
```

---

## Final Checklist

### Before Submission
- [ ] All code runs without errors
- [ ] Demo completes successfully
- [ ] Token usage logged and under $10 total
- [ ] README.md complete and accurate
- [ ] tokens_report.md generated
- [ ] Code commented where non-obvious
- [ ] .env.example file created (without actual key)
- [ ] Git repository initialized
- [ ] .gitignore includes .env, data/*.jsonl

### Repository Structure
```
job-search/
├── .env.example
├── .gitignore
├── requirements.txt
├── README.md
├── tokens_report.md
├── demo.py
├── tests.py (optional)
├── data/
│   └── jobs.jsonl (downloaded)
└── src/
    ├── __init__.py
    ├── data_loader.py
    ├── search_engine.py
    ├── refinement_engine.py
    └── utils.py
```

### .gitignore
```
.env
data/*.jsonl
__pycache__/
*.pyc
.DS_Store
venv/
```

### Submission
1. Create private GitHub repository
2. Push all code
3. Invite: hamedn, alimir1
4. Email hamed@hiring.cafe and ali@hiring.cafe with:
   - Repository link
   - Time spent: ~X hours
   - Brief summary of approach
   - Any assumptions or limitations

---

## Implementation Notes for Agent

### Key Success Factors
1. **Start with data exploration** - understand structure before coding
2. **Test incrementally** - validate each component works before moving on
3. **Monitor tokens religiously** - add tracking from the start
4. **Use provided embeddings** - don't regenerate them (huge cost)
5. **Keep it simple** - no over-engineering, focus on core functionality

### Common Pitfalls to Avoid
- ❌ Re-embedding the dataset (they're already provided!)
- ❌ Passing full job descriptions to LLM (use summaries)
- ❌ Using GPT-4 instead of GPT-4o-mini
- ❌ Re-ranking every query (only when needed)
- ❌ Loading entire dataset into memory multiple times
- ❌ Not tracking tokens during development

### Debugging Tips
- Print token usage after every LLM call
- Test with small dataset first (first 1000 jobs)
- Validate embeddings are correct shape (1536)
- Check filter logic with edge cases
- Compare results across different queries

### Time Allocation Recommendation
- 20% - Data loading & exploration
- 35% - Search engine implementation
- 20% - Refinement engine
- 15% - Demo & testing
- 10% - Documentation

---

## Expected Outcomes

### Token Budget Targets
- **Total development:** <$7
- **Demo run:** <$1
- **Per search query:** 150-500 tokens
- **Per refinement:** 200-400 tokens

### Result Quality Benchmarks
- Top 3 results should be clearly relevant
- Filters should work correctly (remote, seniority, etc.)
- Refinements should narrow results appropriately
- No crashes or errors during demo

### Code Quality
- Clean, readable code with comments
- Proper error handling for missing data
- Efficient numpy operations
- Modular design (easy to extend)

---

This implementation plan provides complete specifications for building a token-efficient, high-quality job search system. Follow the steps sequentially, test incrementally, and monitor token usage throughout development.
